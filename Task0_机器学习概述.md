# Task0 æœºå™¨å­¦ä¹ æ¦‚è¿°

## 1ã€æœºå™¨å­¦ä¹ åˆ†ç±»

##### 1.1ç›‘ç£å­¦ä¹ ï¼ˆsupervised learningï¼‰

Applications in which the training data comprises examples of the input vectors along with their corresponding target vectors are known as *supervised learning* problems.

ç›‘ç£å­¦ä¹ çš„è®­ç»ƒé›†è¦æ±‚åŒ…æ‹¬è¾“å…¥å’Œè¾“å‡º

ä¸»è¦åº”ç”¨äºåˆ†ç±»å’Œé¢„æµ‹ã€‚å¸¸è§çš„ç›‘ç£å­¦ä¹ ç®—æ³•åŒ…æ‹¬å›å½’åˆ†æå’Œç»Ÿè®¡åˆ†ç±»ã€‚

##### 1.2éç›‘ç£å­¦ä¹ ï¼ˆunsupervised learningï¼‰

In other pattern recognition problems, the training data consists of a set of input vectors x without any corresponding target values.

åœ¨éç›‘ç£å­¦ä¹ ä¸­ï¼Œæ— é¡»å¯¹æ•°æ®é›†è¿›è¡Œæ ‡è®°ï¼Œå³æ²¡æœ‰è¾“å‡ºã€‚å…¶éœ€è¦ä»æ•°æ®é›†ä¸­å‘ç°éšå«çš„æŸç§ç»“æ„ï¼Œä»è€Œè·å¾—æ ·æœ¬æ•°æ®çš„ç»“æ„ç‰¹å¾ï¼Œåˆ¤æ–­å“ªäº›æ•°æ®æ¯”è¾ƒç›¸ä¼¼ã€‚

æ— ç›‘ç£å­¦ä¹ çš„å…¸å‹ç®—æ³•æœ‰è‡ªåŠ¨ç¼–ç å™¨ã€å—é™ç»å°”å…¹æ›¼æœºã€æ·±åº¦ç½®ä¿¡ç½‘ç»œç­‰ï¼›å…¸å‹åº”ç”¨æœ‰ï¼šèšç±»å’Œå¼‚å¸¸æ£€æµ‹ç­‰ã€‚

##### 1.3åŠç›‘ç£å­¦ä¹ 

åŠç›‘ç£å­¦ä¹ æ˜¯ç›‘ç£å­¦ä¹ å’Œéç›‘ç£å­¦ä¹ çš„ç»“åˆï¼Œå…¶åœ¨è®­ç»ƒé˜¶æ®µä½¿ç”¨çš„æ˜¯æœªæ ‡è®°çš„æ•°æ®å’Œå·²æ ‡è®°çš„æ•°æ®ï¼Œä¸ä»…è¦å­¦ä¹ å±æ€§ä¹‹é—´çš„ç»“æ„å…³ç³»ï¼Œä¹Ÿè¦è¾“å‡ºåˆ†ç±»æ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚ä¸ä½¿ç”¨æ‰€æœ‰æ ‡ç­¾æ•°æ®çš„æ¨¡å‹ç›¸æ¯”ï¼Œä½¿ç”¨è®­ç»ƒé›†çš„è®­ç»ƒæ¨¡å‹åœ¨è®­ç»ƒæ—¶å¯ä»¥æ›´ä¸ºå‡†ç¡®ï¼Œè€Œä¸”è®­ç»ƒæˆæœ¬æ›´ä½ï¼Œåœ¨å®é™…è¿ç”¨ä¸­ä¹Ÿæ›´ä¸ºæ™®éã€‚

##### 1.4å¼ºåŒ–å­¦ä¹ ï¼ˆreinforcement learningï¼‰

Here the learning algorithm is not given examples of optimal outputs, in contrast to supervised learning, but must instead discover them by a process of trial and error

ç”¨äºæè¿°å’Œè§£å†³æ™ºèƒ½ä½“ï¼ˆagentï¼‰åœ¨ä¸ç¯å¢ƒçš„äº¤äº’è¿‡ç¨‹ä¸­é€šè¿‡å­¦ä¹ ç­–ç•¥ä»¥è¾¾æˆå›æŠ¥æœ€å¤§åŒ–æˆ–å®ç°ç‰¹å®šç›®æ ‡çš„é—®é¢˜. 

## 2.æ³›åŒ–èƒ½åŠ›ï¼ˆgeneralizationï¼‰ä¸è¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰

##### 2.1 å®šä¹‰

The ability to categorize correctly new examples that differ from those used for training is known as *generalization*.

Overfitting is the tendency of data mining procedures to tailor models to the training data, at the expense of generalization to previously unseen data points

##### 2.2 biaså’Œvariance

total error = bias + variance

varianceï¼šç»„é—´æ–¹å·®

biasï¼šå®é™…è¯¯å·®

<img src="https://pic3.zhimg.com/80/v2-e20cd1183ec930a3edc94b30274be29e_hd.jpg">

##### 2.3 è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ

**æ¬ æ‹Ÿåˆ**ä¸€èˆ¬è¡¨ç¤ºæ¨¡å‹å¯¹æ•°æ®çš„è¡¨ç°èƒ½åŠ›ä¸è¶³ï¼Œé€šå¸¸æ˜¯æ¨¡å‹çš„å¤æ‚åº¦ä¸å¤Ÿï¼Œå¹¶ä¸”Biasé«˜ï¼Œè®­ç»ƒé›†çš„æŸå¤±å€¼é«˜ï¼Œæµ‹è¯•é›†çš„æŸå¤±å€¼ä¹Ÿé«˜.

**è¿‡æ‹Ÿåˆ**ä¸€èˆ¬è¡¨ç¤ºæ¨¡å‹å¯¹æ•°æ®çš„è¡¨ç°èƒ½åŠ›è¿‡å¥½ï¼Œé€šå¸¸æ˜¯æ¨¡å‹çš„å¤æ‚åº¦è¿‡é«˜ï¼Œå¹¶ä¸”Varianceé«˜ï¼Œè®­ç»ƒé›†çš„æŸå¤±å€¼ä½ï¼Œæµ‹è¯•é›†çš„æŸå¤±å€¼é«˜.

<img src="https://pic1.zhimg.com/80/v2-22287dec5b6205a5cd45cf6c24773aac_hd.jpg">

##### 2.4 ä¸€èˆ¬è§£å†³æ–¹æ¡ˆ

é«˜varianceï¼šå¢åŠ è®­ç»ƒæ ·æœ¬ã€å‡å°‘ç‰¹å¾ç»´æ•°ã€å‡å°æ¨¡å‹å¤æ‚åº¦

é«˜biasï¼šå¢åŠ ç‰¹å¾ç»´æ•°ã€å¢åŠ æ¨¡å‹å¤æ‚åº¦

## 3.æœºå™¨å­¦ä¹ å¸¸è§ç®—æ³•

#### 3.1å¯¼å›¾

<img src="https://blog.griddynamics.com/content/images/2018/04/machinelearningalgorithms.png">

#### 3.2 å¸¸è§ç®—æ³•ä»‹ç»

##### 3.2.1 Linear Algorithms

https://mp.weixin.qq.com/s/MOuC5nNiv0GKvFv3hIC77A

1. Linear Regression
2. Lasso Regression 
3. Ridge Regression
4. Logistic Regression

##### 3.2.2 Decision Tree

1. ID3
2. C4.5
3. CART

##### 3.2.3 SVM æ”¯æŒå‘é‡æœº

##### 3.2.4 Naive Bayes Algorithms

1. Naive Bayes
2. Gaussian Naive Bayes
3. Multinomial Naive Bayes
4. Bayesian Belief Network (BBN)
5. Bayesian Network (BN)

##### 3.2.5 kNN

##### 3.2.6 Clustering Algorithms

1.  k-Means
2.  k-Medians
3.  Expectation Maximisation (EM)
4.  Hierarchical Clustering

##### 3.2.7 K-Means

##### 3.2.8 Random Forest

##### 3.2.9 Dimensionality Reduction Algorithms

##### 3.2.10 Gradient Boosting algorithms

1. GBM

2. XGBoost

3. LightGBM

4. CatBoost

   ##### 3.2.11Deep Learning Algorithms

1.  Convolutional Neural Network (CNN)
2.  Recurrent Neural Networks (RNNs)
3.  Long Short-Term Memory Networks (LSTMs)
4.  Stacked Auto-Encoders
5.  Deep Boltzmann Machine (DBM)
6.  Deep Belief Networks (DBN)

## 4.æœºå™¨å­¦ä¹ å¸¸è§æ¨¡å‹è¯„ä»·æŒ‡æ ‡

1. MSE(Mean Squared Error)

   å‡æ–¹è¯¯å·®æ˜¯æŒ‡å‚æ•°ä¼°è®¡å€¼ä¸å‚æ•°çœŸå€¼ä¹‹å·®å¹³æ–¹çš„æœŸæœ›å€¼; MSEå¯ä»¥è¯„ä»·æ•°æ®çš„å˜åŒ–ç¨‹åº¦ï¼ŒMSEçš„å€¼è¶Šå°ï¼Œè¯´æ˜é¢„æµ‹æ¨¡å‹æè¿°å®éªŒæ•°æ®å…·æœ‰æ›´å¥½çš„ç²¾ç¡®åº¦ã€‚ï¼ˆ ğ‘–i è¡¨ç¤ºç¬¬ ğ‘–i ä¸ªæ ·æœ¬ï¼Œğ‘N è¡¨ç¤ºæ ·æœ¬æ€»æ•°ï¼‰é€šå¸¸ç”¨æ¥åšå›å½’é—®é¢˜çš„ä»£ä»·å‡½æ•°ã€‚

$$
MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}(y-f(x))^2
$$

2. MAE(Mean Absolute Error)

$$
MSE(y,f(x))=\frac{1}{N}\sum_{i=1}^{N}|y-f(x)|
$$

3. RMSE(Root Mean Squard Error)

$$
RMSE(y,f(x))=\frac{1}{1+MSE(y,f(x))}
$$

4. Top-kå‡†ç¡®ç‡

$$
Top_k(y,pre_y)=\begin{cases}
1, {y \in pre_y}  \\
0, {y \notin pre_y}
\end{cases}
$$

5. æ··æ·†çŸ©é˜µ

   * çœŸæ­£ä¾‹(True Positive, TP):çœŸå®ç±»åˆ«ä¸ºæ­£ä¾‹, é¢„æµ‹ç±»åˆ«ä¸ºæ­£ä¾‹
   * å‡è´Ÿä¾‹(False Negative, FN): çœŸå®ç±»åˆ«ä¸ºæ­£ä¾‹, é¢„æµ‹ç±»åˆ«ä¸ºè´Ÿä¾‹
   * å‡æ­£ä¾‹(False Positive, FP): çœŸå®ç±»åˆ«ä¸ºè´Ÿä¾‹, é¢„æµ‹ç±»åˆ«ä¸ºæ­£ä¾‹ 
   * çœŸè´Ÿä¾‹(True Negative, TN): çœŸå®ç±»åˆ«ä¸ºè´Ÿä¾‹, é¢„æµ‹ç±»åˆ«ä¸ºè´Ÿä¾‹

   * çœŸæ­£ç‡(True Positive Rate, TPR): è¢«é¢„æµ‹ä¸ºæ­£çš„æ­£æ ·æœ¬æ•° / æ­£æ ·æœ¬å®é™…æ•°

   $$
   TPR=\frac{TP}{TP+FN}
   $$

   * å‡è´Ÿç‡(False Negative Rate, FNR): è¢«é¢„æµ‹ä¸ºè´Ÿçš„æ­£æ ·æœ¬æ•°/æ­£æ ·æœ¬å®é™…æ•°

   $$
   FNR=\frac{FN}{TP+FN}
   $$

   * å‡æ­£ç‡(False Positive Rate, FPR): è¢«é¢„æµ‹ä¸ºæ­£çš„è´Ÿæ ·æœ¬æ•°/è´Ÿæ ·æœ¬å®é™…æ•°ï¼Œ

   $$
   FPR=\frac{FP}{FP+TN}
   $$

   * çœŸè´Ÿç‡(True Negative Rate, TNR): è¢«é¢„æµ‹ä¸ºè´Ÿçš„è´Ÿæ ·æœ¬æ•°/è´Ÿæ ·æœ¬å®é™…æ•°ï¼Œ

   $$
   TNR=\frac{TN}{FP+TN}
   $$

   * å‡†ç¡®ç‡(Accuracy)

   $$
   ACC=\frac{TP+TN}{TP+FN+FP+TN}
   $$

   * ç²¾å‡†ç‡

   $$
   P=\frac{TP}{TP+FP}
   $$

   * å¬å›ç‡

   $$
   R=\frac{TP}{TP+FN}
   $$

   * F1-Score

   $$
   \frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}
   $$

   * ROC

   ROCæ›²çº¿çš„æ¨ªè½´ä¸ºâ€œå‡æ­£ä¾‹ç‡â€ï¼Œçºµè½´ä¸ºâ€œçœŸæ­£ä¾‹ç‡â€. ä»¥FPRä¸ºæ¨ªåæ ‡ï¼ŒTPRä¸ºçºµåæ ‡ï¼Œé‚£ä¹ˆROCæ›²çº¿å°±æ˜¯æ”¹å˜å„ç§é˜ˆå€¼åå¾—åˆ°çš„æ‰€æœ‰åæ ‡ç‚¹ (FPR,TPR) çš„è¿çº¿ï¼Œç”»å‡ºæ¥å¦‚ä¸‹ã€‚çº¢çº¿æ˜¯éšæœºä¹±çŒœæƒ…å†µä¸‹çš„ROCï¼Œæ›²çº¿è¶Šé å·¦ä¸Šè§’ï¼Œåˆ†ç±»å™¨è¶Šä½³. 


   * AUC(Area Under Curve)

   AUCå°±æ˜¯ROCæ›²çº¿ä¸‹çš„é¢ç§¯. çœŸå®æƒ…å†µä¸‹ï¼Œç”±äºæ•°æ®æ˜¯ä¸€ä¸ªä¸€ä¸ªçš„ï¼Œé˜ˆå€¼è¢«ç¦»æ•£åŒ–ï¼Œå‘ˆç°çš„æ›²çº¿ä¾¿æ˜¯é”¯é½¿çŠ¶çš„ï¼Œå½“ç„¶æ•°æ®è¶Šå¤šï¼Œé˜ˆå€¼åˆ†çš„è¶Šç»†ï¼Œâ€æ›²çº¿â€è¶Šå…‰æ»‘. 

   

## 5.æœºå™¨å­¦ä¹ æ¨¡å‹é€‰æ‹©

##### 5.1äº¤å‰éªŒè¯

æ‰€æœ‰æ•°æ®åˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼šè®­ç»ƒé›†ã€äº¤å‰éªŒè¯é›†å’Œæµ‹è¯•é›†

##### 5.2 k-æŠ˜å äº¤å‰éªŒè¯

![image-20200108224943019](/Users/sissi/Library/Application Support/typora-user-images/image-20200108224943019.png)

- è®¾è®­ç»ƒé›†ä¸ºS ï¼Œå°†è®­ç»ƒé›†ç­‰åˆ†ä¸ºkä»½:$\{S_1, S_2, ..., S_k\}$. 
- ç„¶åæ¯æ¬¡ä»é›†åˆä¸­æ‹¿å‡ºk-1ä»½è¿›è¡Œè®­ç»ƒ
- åˆ©ç”¨é›†åˆä¸­å‰©ä¸‹çš„é‚£ä¸€ä»½æ¥è¿›è¡Œæµ‹è¯•å¹¶è®¡ç®—æŸå¤±å€¼
- æœ€åå¾—åˆ°kæ¬¡æµ‹è¯•å¾—åˆ°çš„æŸå¤±å€¼ï¼Œå¹¶é€‰æ‹©å¹³å‡æŸå¤±å€¼æœ€å°çš„æ¨¡å‹

##### 5.3 nested cross validation

![image-20200108225144444](/Users/sissi/Library/Application Support/typora-user-images/image-20200108225144444.png)

![](/Users/sissi/Documents/æœºå™¨å­¦ä¹ /å¾®ä¿¡å­¦ä¹ å°ç»„/team-learning-master/åˆçº§ç®—æ³•æ¢³ç†/502650-20171215192759371-739543867.png)



å‚è€ƒä¹¦ï¼š

æŒ‡æ ‡ï¼šhttps://www.cnblogs.com/lliuye/p/9549881.html

â€‹          https://blog.csdn.net/qq_20011607/article/details/81712811